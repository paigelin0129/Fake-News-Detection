{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-18T02:26:09.197966Z",
     "start_time": "2024-11-18T02:25:49.541347Z"
    }
   },
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "import gensim\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import tensorflow_hub as hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  label\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...      1\n",
      "1   Drunk Bragging Trump Staffer Started Russian ...      1\n",
      "2   Sheriff David Clarke Becomes An Internet Joke...      1\n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...      1\n",
      "4   Pope Francis Just Called Out Donald Trump Dur...      1\n",
      "title    0\n",
      "label    0\n",
      "dtype: int64\n",
      "title    0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "df_fake = pd.read_csv('data/Fake.csv')\n",
    "df_real = pd.read_csv('data/True.csv')\n",
    "\n",
    "# label the data, 1 for fake news, 0 for real news\n",
    "df_fake['label'] = 1\n",
    "df_real['label'] = 0\n",
    "\n",
    "# combine the data\n",
    "df = pd.concat([df_fake, df_real], axis=0).reset_index(drop=True)\n",
    "\n",
    "# drop useless columns\n",
    "df = df.drop(['subject', 'date', 'text'], axis=1)\n",
    "\n",
    "# check the data validity\n",
    "print(df.head())\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# load another test data\n",
    "df_fnn = pd.read_csv('data/FakeNewsNet.csv')\n",
    "df_fnn = df_fnn.drop(['news_url', 'source_domain', 'tweet_num'], axis=1)\n",
    "\n",
    "# rename 'real' column to 'label' column\n",
    "df_fnn = df_fnn.rename(columns={'real': 'label'})\n",
    "\n",
    "print(df_fnn.isnull().sum())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T15:47:12.386077Z",
     "start_time": "2024-11-17T15:47:11.431275Z"
    }
   },
   "id": "baa19d68cbad9471"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# customize NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# add patterns for donald trump, hillary clinton, barack obama, joe biden, democrat, republican\n",
    "patterns = [\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"donald\"}, {\"LOWER\": \"trump\"}], \"id\": \"donald_trump\"},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"trump\"}], \"id\": \"donald_trump\"},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"donald\"}], \"id\": \"donald_trump\"},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"hillary\"}, {\"LOWER\": \"clinton\"}], \"id\": \"hillary_clinton\"},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"clinton\"}], \"id\": \"hillary_clinton\"},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"hillary\"}], \"id\": \"hillary_clinton\"},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"barack\"}, {\"LOWER\": \"obama\"}], \"id\": \"barack_obama\"},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"obama\"}], \"id\": \"barack_obama\"},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"obamas\"}], \"id\": \"barack_obama\"},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"barack\"}], \"id\": \"barack_obama\"},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"joe\"}, {\"LOWER\": \"biden\"}], \"id\": \"joe_biden\"},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"biden\"}], \"id\": \"joe_biden\"},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"joe\"}], \"id\": \"joe_biden\"},\n",
    "    {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"democrat\"}], \"id\": \"democrat\"},\n",
    "    {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"democrats\"}], \"id\": \"democrat\"},\n",
    "    {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"republican\"}], \"id\": \"republican\"},\n",
    "    {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"republicans\"}], \"id\": \"republican\"},\n",
    "    {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"gop\"}], \"id\": \"republican\"},\n",
    "]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "counter = defaultdict(int)\n",
    "\n",
    "def preprocess_text_with_ner(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # tokenize\n",
    "    tokens = []\n",
    "    words = nlp(text)\n",
    "    \n",
    "    # remove stopwords and lemmatize words\n",
    "    # substitute named entities with their labels for person, organization, date, time, and location\n",
    "    for word in words:\n",
    "        if word.is_stop or len(word.text)<=1 or word.is_space:\n",
    "            continue\n",
    "        if word.ent_type_ in ['PERSON', 'ORG', 'DATE', 'TIME', 'GPE']:\n",
    "            tokens.append(\"<\" + word.ent_type_ + \">\")\n",
    "        else:\n",
    "            tokens.append(word.lemma_)\n",
    "            counter[word.lemma_] += 1\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # remove D\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # tokenize\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # remove stopwords and lemmatize words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if (word not in stop_words and word != '')]\n",
    "    text = ' '.join(lemmatized_words)\n",
    "    return text\n",
    "\n",
    "df['title_clean'] = df['title'].apply(preprocess_text)\n",
    "df['title_clean_ner'] = df['title'].apply(preprocess_text_with_ner)\n",
    "\n",
    "df_fnn['title_clean'] = df_fnn['title'].apply(preprocess_text)\n",
    "df_fnn['title_clean_ner'] = df_fnn['title'].apply(preprocess_text_with_ner)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T15:51:29.036469Z",
     "start_time": "2024-11-17T15:47:18.943412Z"
    }
   },
   "id": "7540805a4bd6bc98"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# save current df to a file for later use\n",
    "df.to_csv('data/processed_data.csv', index=False)\n",
    "df_fnn.to_csv('data/processed_data_fnn.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T15:52:08.752423Z",
     "start_time": "2024-11-17T15:52:08.551496Z"
    }
   },
   "id": "9fe4ea05f10c7b0c"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# load the processed data\n",
    "df = pd.read_csv('data/processed_data.csv')\n",
    "df_fnn = pd.read_csv('data/processed_data_fnn.csv')\n",
    "\n",
    "# drop rows with empty value\n",
    "df = df.dropna()\n",
    "df_fnn = df_fnn.dropna()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T15:54:14.295905Z",
     "start_time": "2024-11-17T15:54:14.143780Z"
    }
   },
   "id": "148a8379dd6f8d25"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# common function to train and test models\n",
    "# use method to specify the feature extraction method\n",
    "def train_and_test(method, X_train, X_test, y_train, y_test):\n",
    "    models = [\n",
    "        LogisticRegression(),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        SVC(),\n",
    "    ]\n",
    "    \n",
    "    print(f'Feature selection method: {method}')\n",
    "\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"{model.__class__.__name__}: {accuracy*100:.2f}\")\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        print(report)\n",
    "        print(\"-\"*30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T15:54:16.161121Z",
     "start_time": "2024-11-17T15:54:16.158028Z"
    }
   },
   "id": "eed4ca17dd59f0c5"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# Use Bag of Words and TF-IDF to extract features\n",
    "def train_and_test_bow(title_column, k):\n",
    "    # For title_clean\n",
    "    X = df[title_column]\n",
    "    y = df['label']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5664)\n",
    "    \n",
    "    # Bag of Words\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train_bow = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    # perform chi2 feature selection and select top 100 features\n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    X_train_bow = selector.fit_transform(X_train_bow, y_train)\n",
    "    \n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "    X_test_bow = selector.transform(X_test_bow)\n",
    "    \n",
    "    print(f'For {title_column} (original data):')\n",
    "    train_and_test('Bag of Words', X_train_bow, X_test_bow, y_train, y_test)\n",
    "    \n",
    "    # use same model and test on df_fnn\n",
    "    X_test = df_fnn[title_column]\n",
    "    y_test = df_fnn['label']\n",
    "    \n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "    X_test_bow = selector.transform(X_test_bow)\n",
    "    \n",
    "    print(f'For {title_column} (FakeNewsNet data):')\n",
    "    train_and_test('Bag of Words', X_train_bow, X_test_bow, y_train, y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T16:32:09.009293Z",
     "start_time": "2024-11-17T16:32:09.004170Z"
    }
   },
   "id": "c0e194fd134a337b"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "def train_and_test_tfidf(title_column, k):\n",
    "    X = df[title_column]\n",
    "    y = df['label']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5664)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    # perform chi2 feature selection and select top 100 features\n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    X_train_tfidf = selector.fit_transform(X_train_tfidf, y_train)\n",
    "    \n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    X_test_tfidf = selector.transform(X_test_tfidf)\n",
    "    \n",
    "    print(f'For {title_column} (original data):')\n",
    "    train_and_test('TF-IDF', X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "    \n",
    "    # use same model and test on df_fnn\n",
    "    X_test = df_fnn[title_column]\n",
    "    y_test = df_fnn['label']\n",
    "    \n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    X_test_tfidf = selector.transform(X_test_tfidf)\n",
    "    \n",
    "    print(f'For {title_column} (FakeNewsNet data):')\n",
    "    train_and_test('TF-IDF', X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T16:32:36.810212Z",
     "start_time": "2024-11-17T16:32:36.802624Z"
    }
   },
   "id": "5c7db23af9dfd052"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For title_clean (original data):\n",
      "Feature selection method: Bag of Words\n",
      "LogisticRegression: 84.52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85      4258\n",
      "           1       0.93      0.77      0.84      4720\n",
      "\n",
      "    accuracy                           0.85      8978\n",
      "   macro avg       0.85      0.85      0.84      8978\n",
      "weighted avg       0.86      0.85      0.84      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 84.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85      4258\n",
      "           1       0.93      0.76      0.84      4720\n",
      "\n",
      "    accuracy                           0.84      8978\n",
      "   macro avg       0.85      0.85      0.84      8978\n",
      "weighted avg       0.86      0.84      0.84      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 84.61\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85      4258\n",
      "           1       0.93      0.77      0.84      4720\n",
      "\n",
      "    accuracy                           0.85      8978\n",
      "   macro avg       0.86      0.85      0.85      8978\n",
      "weighted avg       0.86      0.85      0.85      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 84.70\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.93      0.85      4258\n",
      "           1       0.92      0.77      0.84      4720\n",
      "\n",
      "    accuracy                           0.85      8978\n",
      "   macro avg       0.86      0.85      0.85      8978\n",
      "weighted avg       0.86      0.85      0.85      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean (FakeNewsNet data):\n",
      "Feature selection method: Bag of Words\n",
      "LogisticRegression: 36.13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.78      0.38      5754\n",
      "           1       0.75      0.22      0.34     17438\n",
      "\n",
      "    accuracy                           0.36     23192\n",
      "   macro avg       0.50      0.50      0.36     23192\n",
      "weighted avg       0.63      0.36      0.35     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 35.43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.80      0.38      5754\n",
      "           1       0.76      0.21      0.33     17438\n",
      "\n",
      "    accuracy                           0.35     23192\n",
      "   macro avg       0.50      0.50      0.35     23192\n",
      "weighted avg       0.63      0.35      0.34     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 35.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.80      0.38      5754\n",
      "           1       0.76      0.21      0.33     17438\n",
      "\n",
      "    accuracy                           0.35     23192\n",
      "   macro avg       0.50      0.50      0.35     23192\n",
      "weighted avg       0.63      0.35      0.34     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 35.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.79      0.38      5754\n",
      "           1       0.75      0.21      0.33     17438\n",
      "\n",
      "    accuracy                           0.35     23192\n",
      "   macro avg       0.50      0.50      0.35     23192\n",
      "weighted avg       0.63      0.35      0.34     23192\n",
      "\n",
      "------------------------------\n",
      "For title_clean (original data):\n",
      "Feature selection method: Bag of Words\n",
      "LogisticRegression: 89.34\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89      4258\n",
      "           1       0.93      0.86      0.89      4720\n",
      "\n",
      "    accuracy                           0.89      8978\n",
      "   macro avg       0.89      0.90      0.89      8978\n",
      "weighted avg       0.90      0.89      0.89      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 87.86\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      4258\n",
      "           1       0.92      0.84      0.88      4720\n",
      "\n",
      "    accuracy                           0.88      8978\n",
      "   macro avg       0.88      0.88      0.88      8978\n",
      "weighted avg       0.88      0.88      0.88      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 89.13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89      4258\n",
      "           1       0.93      0.86      0.89      4720\n",
      "\n",
      "    accuracy                           0.89      8978\n",
      "   macro avg       0.89      0.89      0.89      8978\n",
      "weighted avg       0.89      0.89      0.89      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 89.52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89      4258\n",
      "           1       0.94      0.86      0.90      4720\n",
      "\n",
      "    accuracy                           0.90      8978\n",
      "   macro avg       0.90      0.90      0.90      8978\n",
      "weighted avg       0.90      0.90      0.90      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean (FakeNewsNet data):\n",
      "Feature selection method: Bag of Words\n",
      "LogisticRegression: 45.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.55      0.34      5754\n",
      "           1       0.74      0.43      0.54     17438\n",
      "\n",
      "    accuracy                           0.46     23192\n",
      "   macro avg       0.49      0.49      0.44     23192\n",
      "weighted avg       0.62      0.46      0.49     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 42.88\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.59      0.34      5754\n",
      "           1       0.74      0.37      0.50     17438\n",
      "\n",
      "    accuracy                           0.43     23192\n",
      "   macro avg       0.49      0.48      0.42     23192\n",
      "weighted avg       0.61      0.43      0.46     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 44.20\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.58      0.34      5754\n",
      "           1       0.74      0.40      0.52     17438\n",
      "\n",
      "    accuracy                           0.44     23192\n",
      "   macro avg       0.49      0.49      0.43     23192\n",
      "weighted avg       0.62      0.44      0.47     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 44.38\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.57      0.34      5754\n",
      "           1       0.74      0.40      0.52     17438\n",
      "\n",
      "    accuracy                           0.44     23192\n",
      "   macro avg       0.49      0.49      0.43     23192\n",
      "weighted avg       0.62      0.44      0.48     23192\n",
      "\n",
      "------------------------------\n",
      "For title_clean (original data):\n",
      "Feature selection method: TF-IDF\n",
      "LogisticRegression: 83.87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.95      0.85      4258\n",
      "           1       0.94      0.74      0.83      4720\n",
      "\n",
      "    accuracy                           0.84      8978\n",
      "   macro avg       0.85      0.84      0.84      8978\n",
      "weighted avg       0.86      0.84      0.84      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 83.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.93      0.84      4258\n",
      "           1       0.92      0.74      0.82      4720\n",
      "\n",
      "    accuracy                           0.83      8978\n",
      "   macro avg       0.84      0.84      0.83      8978\n",
      "weighted avg       0.85      0.83      0.83      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 84.07\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85      4258\n",
      "           1       0.94      0.75      0.83      4720\n",
      "\n",
      "    accuracy                           0.84      8978\n",
      "   macro avg       0.85      0.85      0.84      8978\n",
      "weighted avg       0.86      0.84      0.84      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 84.41\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.95      0.85      4258\n",
      "           1       0.94      0.75      0.83      4720\n",
      "\n",
      "    accuracy                           0.84      8978\n",
      "   macro avg       0.86      0.85      0.84      8978\n",
      "weighted avg       0.86      0.84      0.84      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean (FakeNewsNet data):\n",
      "Feature selection method: TF-IDF\n",
      "LogisticRegression: 34.43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.82      0.38      5754\n",
      "           1       0.76      0.19      0.30     17438\n",
      "\n",
      "    accuracy                           0.34     23192\n",
      "   macro avg       0.50      0.50      0.34     23192\n",
      "weighted avg       0.63      0.34      0.32     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 33.09\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.82      0.38      5754\n",
      "           1       0.74      0.17      0.27     17438\n",
      "\n",
      "    accuracy                           0.33     23192\n",
      "   macro avg       0.49      0.50      0.33     23192\n",
      "weighted avg       0.62      0.33      0.30     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 33.26\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.82      0.38      5754\n",
      "           1       0.74      0.17      0.28     17438\n",
      "\n",
      "    accuracy                           0.33     23192\n",
      "   macro avg       0.50      0.50      0.33     23192\n",
      "weighted avg       0.62      0.33      0.30     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 34.49\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.81      0.38      5754\n",
      "           1       0.76      0.19      0.30     17438\n",
      "\n",
      "    accuracy                           0.34     23192\n",
      "   macro avg       0.50      0.50      0.34     23192\n",
      "weighted avg       0.63      0.34      0.32     23192\n",
      "\n",
      "------------------------------\n",
      "For title_clean (original data):\n",
      "Feature selection method: TF-IDF\n",
      "LogisticRegression: 88.72\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89      4258\n",
      "           1       0.94      0.84      0.89      4720\n",
      "\n",
      "    accuracy                           0.89      8978\n",
      "   macro avg       0.89      0.89      0.89      8978\n",
      "weighted avg       0.89      0.89      0.89      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 88.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      4258\n",
      "           1       0.92      0.85      0.88      4720\n",
      "\n",
      "    accuracy                           0.88      8978\n",
      "   macro avg       0.88      0.88      0.88      8978\n",
      "weighted avg       0.88      0.88      0.88      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 89.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89      4258\n",
      "           1       0.94      0.85      0.89      4720\n",
      "\n",
      "    accuracy                           0.89      8978\n",
      "   macro avg       0.89      0.89      0.89      8978\n",
      "weighted avg       0.89      0.89      0.89      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 89.52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89      4258\n",
      "           1       0.94      0.86      0.90      4720\n",
      "\n",
      "    accuracy                           0.90      8978\n",
      "   macro avg       0.90      0.90      0.90      8978\n",
      "weighted avg       0.90      0.90      0.90      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean (FakeNewsNet data):\n",
      "Feature selection method: TF-IDF\n",
      "LogisticRegression: 45.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.59      0.35      5754\n",
      "           1       0.75      0.42      0.54     17438\n",
      "\n",
      "    accuracy                           0.46     23192\n",
      "   macro avg       0.50      0.50      0.44     23192\n",
      "weighted avg       0.63      0.46      0.49     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 42.39\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.62      0.35      5754\n",
      "           1       0.74      0.36      0.48     17438\n",
      "\n",
      "    accuracy                           0.42     23192\n",
      "   macro avg       0.49      0.49      0.42     23192\n",
      "weighted avg       0.62      0.42      0.45     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 42.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.63      0.35      5754\n",
      "           1       0.75      0.36      0.48     17438\n",
      "\n",
      "    accuracy                           0.42     23192\n",
      "   macro avg       0.50      0.49      0.42     23192\n",
      "weighted avg       0.62      0.42      0.45     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 45.21\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.59      0.35      5754\n",
      "           1       0.75      0.41      0.53     17438\n",
      "\n",
      "    accuracy                           0.45     23192\n",
      "   macro avg       0.50      0.50      0.44     23192\n",
      "weighted avg       0.63      0.45      0.48     23192\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test with title_clean\n",
    "train_and_test_bow('title_clean', 100)\n",
    "train_and_test_bow('title_clean', 300)\n",
    "train_and_test_tfidf('title_clean', 100)\n",
    "train_and_test_tfidf('title_clean', 300)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T16:46:40.100946Z",
     "start_time": "2024-11-17T16:44:04.706168Z"
    }
   },
   "id": "64c5388bdfdcbcbf"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For title_clean_ner (original data):\n",
      "Feature selection method: Bag of Words\n",
      "LogisticRegression: 84.36\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85      4258\n",
      "           1       0.90      0.79      0.84      4720\n",
      "\n",
      "    accuracy                           0.84      8978\n",
      "   macro avg       0.85      0.85      0.84      8978\n",
      "weighted avg       0.85      0.84      0.84      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 83.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.84      4258\n",
      "           1       0.90      0.78      0.83      4720\n",
      "\n",
      "    accuracy                           0.83      8978\n",
      "   macro avg       0.84      0.84      0.83      8978\n",
      "weighted avg       0.84      0.83      0.83      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 84.16\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84      4258\n",
      "           1       0.90      0.79      0.84      4720\n",
      "\n",
      "    accuracy                           0.84      8978\n",
      "   macro avg       0.85      0.84      0.84      8978\n",
      "weighted avg       0.85      0.84      0.84      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 83.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.84      4258\n",
      "           1       0.92      0.76      0.83      4720\n",
      "\n",
      "    accuracy                           0.84      8978\n",
      "   macro avg       0.85      0.84      0.84      8978\n",
      "weighted avg       0.85      0.84      0.84      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean_ner (FakeNewsNet data):\n",
      "Feature selection method: Bag of Words\n",
      "LogisticRegression: 49.89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.39      0.28      5754\n",
      "           1       0.73      0.53      0.62     17438\n",
      "\n",
      "    accuracy                           0.50     23192\n",
      "   macro avg       0.47      0.46      0.45     23192\n",
      "weighted avg       0.60      0.50      0.53     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 48.82\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.40      0.28      5754\n",
      "           1       0.72      0.52      0.60     17438\n",
      "\n",
      "    accuracy                           0.49     23192\n",
      "   macro avg       0.47      0.46      0.44     23192\n",
      "weighted avg       0.60      0.49      0.52     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 49.28\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.39      0.28      5754\n",
      "           1       0.72      0.53      0.61     17438\n",
      "\n",
      "    accuracy                           0.49     23192\n",
      "   macro avg       0.47      0.46      0.44     23192\n",
      "weighted avg       0.60      0.49      0.53     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 42.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.56      0.33      5754\n",
      "           1       0.72      0.38      0.49     17438\n",
      "\n",
      "    accuracy                           0.42     23192\n",
      "   macro avg       0.48      0.47      0.41     23192\n",
      "weighted avg       0.60      0.42      0.45     23192\n",
      "\n",
      "------------------------------\n",
      "For title_clean_ner (original data):\n",
      "Feature selection method: Bag of Words\n",
      "LogisticRegression: 88.19\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      4258\n",
      "           1       0.92      0.85      0.88      4720\n",
      "\n",
      "    accuracy                           0.88      8978\n",
      "   macro avg       0.88      0.88      0.88      8978\n",
      "weighted avg       0.89      0.88      0.88      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 85.82\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86      4258\n",
      "           1       0.91      0.81      0.86      4720\n",
      "\n",
      "    accuracy                           0.86      8978\n",
      "   macro avg       0.86      0.86      0.86      8978\n",
      "weighted avg       0.86      0.86      0.86      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 87.57\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.87      4258\n",
      "           1       0.92      0.84      0.88      4720\n",
      "\n",
      "    accuracy                           0.88      8978\n",
      "   macro avg       0.88      0.88      0.88      8978\n",
      "weighted avg       0.88      0.88      0.88      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 87.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      4258\n",
      "           1       0.92      0.84      0.88      4720\n",
      "\n",
      "    accuracy                           0.88      8978\n",
      "   macro avg       0.88      0.88      0.88      8978\n",
      "weighted avg       0.88      0.88      0.88      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean_ner (FakeNewsNet data):\n",
      "Feature selection method: Bag of Words\n",
      "LogisticRegression: 49.44\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.45      0.31      5754\n",
      "           1       0.74      0.51      0.60     17438\n",
      "\n",
      "    accuracy                           0.49     23192\n",
      "   macro avg       0.49      0.48      0.45     23192\n",
      "weighted avg       0.61      0.49      0.53     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 45.70\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.54      0.33      5754\n",
      "           1       0.74      0.43      0.54     17438\n",
      "\n",
      "    accuracy                           0.46     23192\n",
      "   macro avg       0.49      0.48      0.44     23192\n",
      "weighted avg       0.61      0.46      0.49     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 47.41\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.50      0.32      5754\n",
      "           1       0.74      0.47      0.57     17438\n",
      "\n",
      "    accuracy                           0.47     23192\n",
      "   macro avg       0.49      0.48      0.45     23192\n",
      "weighted avg       0.61      0.47      0.51     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 48.08\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.46      0.31      5754\n",
      "           1       0.73      0.49      0.59     17438\n",
      "\n",
      "    accuracy                           0.48     23192\n",
      "   macro avg       0.48      0.47      0.45     23192\n",
      "weighted avg       0.61      0.48      0.52     23192\n",
      "\n",
      "------------------------------\n",
      "For title_clean_ner (original data):\n",
      "Feature selection method: TF-IDF\n",
      "LogisticRegression: 82.87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.83      4258\n",
      "           1       0.87      0.80      0.83      4720\n",
      "\n",
      "    accuracy                           0.83      8978\n",
      "   macro avg       0.83      0.83      0.83      8978\n",
      "weighted avg       0.83      0.83      0.83      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 83.06\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      4258\n",
      "           1       0.86      0.81      0.83      4720\n",
      "\n",
      "    accuracy                           0.83      8978\n",
      "   macro avg       0.83      0.83      0.83      8978\n",
      "weighted avg       0.83      0.83      0.83      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 85.24\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85      4258\n",
      "           1       0.88      0.83      0.86      4720\n",
      "\n",
      "    accuracy                           0.85      8978\n",
      "   macro avg       0.85      0.85      0.85      8978\n",
      "weighted avg       0.85      0.85      0.85      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 83.35\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.84      4258\n",
      "           1       0.92      0.75      0.83      4720\n",
      "\n",
      "    accuracy                           0.83      8978\n",
      "   macro avg       0.84      0.84      0.83      8978\n",
      "weighted avg       0.85      0.83      0.83      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean_ner (FakeNewsNet data):\n",
      "Feature selection method: TF-IDF\n",
      "LogisticRegression: 50.29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.35      0.26      5754\n",
      "           1       0.72      0.55      0.63     17438\n",
      "\n",
      "    accuracy                           0.50     23192\n",
      "   macro avg       0.46      0.45      0.44     23192\n",
      "weighted avg       0.59      0.50      0.54     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 45.33\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.52      0.32      5754\n",
      "           1       0.73      0.43      0.54     17438\n",
      "\n",
      "    accuracy                           0.45     23192\n",
      "   macro avg       0.48      0.48      0.43     23192\n",
      "weighted avg       0.61      0.45      0.49     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 45.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.51      0.32      5754\n",
      "           1       0.73      0.44      0.55     17438\n",
      "\n",
      "    accuracy                           0.45     23192\n",
      "   macro avg       0.48      0.47      0.43     23192\n",
      "weighted avg       0.61      0.45      0.49     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 46.08\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.45      0.29      5754\n",
      "           1       0.72      0.46      0.56     17438\n",
      "\n",
      "    accuracy                           0.46     23192\n",
      "   macro avg       0.47      0.46      0.43     23192\n",
      "weighted avg       0.59      0.46      0.50     23192\n",
      "\n",
      "------------------------------\n",
      "For title_clean_ner (original data):\n",
      "Feature selection method: TF-IDF\n",
      "LogisticRegression: 87.55\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87      4258\n",
      "           1       0.91      0.84      0.88      4720\n",
      "\n",
      "    accuracy                           0.88      8978\n",
      "   macro avg       0.88      0.88      0.88      8978\n",
      "weighted avg       0.88      0.88      0.88      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 85.22\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.85      4258\n",
      "           1       0.87      0.85      0.86      4720\n",
      "\n",
      "    accuracy                           0.85      8978\n",
      "   macro avg       0.85      0.85      0.85      8978\n",
      "weighted avg       0.85      0.85      0.85      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 88.59\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88      4258\n",
      "           1       0.91      0.87      0.89      4720\n",
      "\n",
      "    accuracy                           0.89      8978\n",
      "   macro avg       0.89      0.89      0.89      8978\n",
      "weighted avg       0.89      0.89      0.89      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 88.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88      4258\n",
      "           1       0.93      0.84      0.88      4720\n",
      "\n",
      "    accuracy                           0.88      8978\n",
      "   macro avg       0.88      0.88      0.88      8978\n",
      "weighted avg       0.88      0.88      0.88      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean_ner (FakeNewsNet data):\n",
      "Feature selection method: TF-IDF\n",
      "LogisticRegression: 54.70\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.32      0.26      5754\n",
      "           1       0.74      0.62      0.67     17438\n",
      "\n",
      "    accuracy                           0.55     23192\n",
      "   macro avg       0.48      0.47      0.47     23192\n",
      "weighted avg       0.61      0.55      0.57     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 48.42\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.49      0.32      5754\n",
      "           1       0.74      0.48      0.58     17438\n",
      "\n",
      "    accuracy                           0.48     23192\n",
      "   macro avg       0.49      0.49      0.45     23192\n",
      "weighted avg       0.62      0.48      0.52     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 48.49\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.50      0.33      5754\n",
      "           1       0.74      0.48      0.58     17438\n",
      "\n",
      "    accuracy                           0.48     23192\n",
      "   macro avg       0.49      0.49      0.45     23192\n",
      "weighted avg       0.62      0.48      0.52     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 48.19\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.49      0.32      5754\n",
      "           1       0.74      0.48      0.58     17438\n",
      "\n",
      "    accuracy                           0.48     23192\n",
      "   macro avg       0.49      0.48      0.45     23192\n",
      "weighted avg       0.61      0.48      0.52     23192\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test with title_clean_ner\n",
    "train_and_test_bow('title_clean_ner', 100)\n",
    "train_and_test_bow('title_clean_ner', 300)\n",
    "train_and_test_tfidf('title_clean_ner', 100)\n",
    "train_and_test_tfidf('title_clean_ner', 300)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T16:51:06.391067Z",
     "start_time": "2024-11-17T16:47:47.345904Z"
    }
   },
   "id": "8254b82ac0440d88"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# use word embeddings to extract features\n",
    "word2vec_model = gensim.models.KeyedVectors.load('word2vec-google-news-300.bin')\n",
    "\n",
    "def get_word_vector(word):\n",
    "    try:\n",
    "        return word2vec_model[word]\n",
    "    except:\n",
    "        return np.zeros(300)\n",
    "\n",
    "def get_sentence_vector(sentence):\n",
    "    words = sentence.split()\n",
    "    vectors = [get_word_vector(word) for word in words]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "def train_and_test_word_embedding(title_column):\n",
    "    X = df[title_column]\n",
    "    y = df['label']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5664)\n",
    "    \n",
    "    X_train_word_embedding = np.array([get_sentence_vector(sentence) for sentence in X_train])\n",
    "    X_test_word_embedding = np.array([get_sentence_vector(sentence) for sentence in X_test])\n",
    "    \n",
    "    print(f'For {title_column} (original data):')\n",
    "    train_and_test('Word Embedding', X_train_word_embedding, X_test_word_embedding, y_train, y_test)\n",
    "    \n",
    "    # use same model and test on df_fnn\n",
    "    X_test = df_fnn[title_column]\n",
    "    y_test = df_fnn['label']\n",
    "    \n",
    "    X_test_word_embedding = np.array([get_sentence_vector(sentence) for sentence in X_test])\n",
    "    \n",
    "    print(f'For {title_column} (FakeNewsNet data):')\n",
    "    train_and_test('Word Embedding', X_train_word_embedding, X_test_word_embedding, y_train, y_test)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T16:51:56.518894Z",
     "start_time": "2024-11-17T16:51:54.254640Z"
    }
   },
   "id": "8ae1bfe28deb4e5e"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For title_clean (original data):\n",
      "Feature selection method: Word Embedding\n",
      "LogisticRegression: 90.59\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90      4258\n",
      "           1       0.91      0.91      0.91      4720\n",
      "\n",
      "    accuracy                           0.91      8978\n",
      "   macro avg       0.91      0.91      0.91      8978\n",
      "weighted avg       0.91      0.91      0.91      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 80.74\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79      4258\n",
      "           1       0.80      0.84      0.82      4720\n",
      "\n",
      "    accuracy                           0.81      8978\n",
      "   macro avg       0.81      0.81      0.81      8978\n",
      "weighted avg       0.81      0.81      0.81      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 91.18\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91      4258\n",
      "           1       0.93      0.90      0.91      4720\n",
      "\n",
      "    accuracy                           0.91      8978\n",
      "   macro avg       0.91      0.91      0.91      8978\n",
      "weighted avg       0.91      0.91      0.91      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 93.77\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93      4258\n",
      "           1       0.95      0.93      0.94      4720\n",
      "\n",
      "    accuracy                           0.94      8978\n",
      "   macro avg       0.94      0.94      0.94      8978\n",
      "weighted avg       0.94      0.94      0.94      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean (FakeNewsNet data):\n",
      "Feature selection method: Word Embedding\n",
      "LogisticRegression: 68.30\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.13      0.16      5754\n",
      "           1       0.75      0.87      0.80     17438\n",
      "\n",
      "    accuracy                           0.68     23192\n",
      "   macro avg       0.49      0.50      0.48     23192\n",
      "weighted avg       0.62      0.68      0.65     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 59.27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.27      0.25      5754\n",
      "           1       0.74      0.70      0.72     17438\n",
      "\n",
      "    accuracy                           0.59     23192\n",
      "   macro avg       0.49      0.49      0.48     23192\n",
      "weighted avg       0.62      0.59      0.60     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 63.56\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.18      0.20      5754\n",
      "           1       0.74      0.78      0.76     17438\n",
      "\n",
      "    accuracy                           0.64     23192\n",
      "   macro avg       0.48      0.48      0.48     23192\n",
      "weighted avg       0.61      0.64      0.62     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 61.76\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.21      0.22      5754\n",
      "           1       0.74      0.75      0.75     17438\n",
      "\n",
      "    accuracy                           0.62     23192\n",
      "   macro avg       0.48      0.48      0.48     23192\n",
      "weighted avg       0.61      0.62      0.62     23192\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test with title_clean\n",
    "train_and_test_word_embedding('title_clean')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T16:58:03.046334Z",
     "start_time": "2024-11-17T16:52:00.004498Z"
    }
   },
   "id": "1d83f0d7542340bb"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For title_clean_ner (original data):\n",
      "Feature selection method: Word Embedding\n",
      "LogisticRegression: 87.47\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87      4258\n",
      "           1       0.89      0.87      0.88      4720\n",
      "\n",
      "    accuracy                           0.87      8978\n",
      "   macro avg       0.87      0.87      0.87      8978\n",
      "weighted avg       0.87      0.87      0.87      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 79.87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78      4258\n",
      "           1       0.79      0.83      0.81      4720\n",
      "\n",
      "    accuracy                           0.80      8978\n",
      "   macro avg       0.80      0.80      0.80      8978\n",
      "weighted avg       0.80      0.80      0.80      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 89.53\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89      4258\n",
      "           1       0.92      0.87      0.90      4720\n",
      "\n",
      "    accuracy                           0.90      8978\n",
      "   macro avg       0.90      0.90      0.90      8978\n",
      "weighted avg       0.90      0.90      0.90      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 91.67\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91      4258\n",
      "           1       0.93      0.91      0.92      4720\n",
      "\n",
      "    accuracy                           0.92      8978\n",
      "   macro avg       0.92      0.92      0.92      8978\n",
      "weighted avg       0.92      0.92      0.92      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean_ner (FakeNewsNet data):\n",
      "Feature selection method: Word Embedding\n",
      "LogisticRegression: 65.63\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.26      0.27      5754\n",
      "           1       0.76      0.79      0.77     17438\n",
      "\n",
      "    accuracy                           0.66     23192\n",
      "   macro avg       0.53      0.52      0.52     23192\n",
      "weighted avg       0.65      0.66      0.65     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 56.51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.38      0.30      5754\n",
      "           1       0.75      0.63      0.68     17438\n",
      "\n",
      "    accuracy                           0.57     23192\n",
      "   macro avg       0.50      0.50      0.49     23192\n",
      "weighted avg       0.63      0.57      0.59     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 61.21\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.28      0.26      5754\n",
      "           1       0.75      0.72      0.74     17438\n",
      "\n",
      "    accuracy                           0.61     23192\n",
      "   macro avg       0.50      0.50      0.50     23192\n",
      "weighted avg       0.63      0.61      0.62     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 58.37\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.33      0.28      5754\n",
      "           1       0.75      0.67      0.71     17438\n",
      "\n",
      "    accuracy                           0.58     23192\n",
      "   macro avg       0.50      0.50      0.49     23192\n",
      "weighted avg       0.63      0.58      0.60     23192\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test with title_clean_ner\n",
    "train_and_test_word_embedding('title_clean_ner')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T17:06:33.081135Z",
     "start_time": "2024-11-17T16:59:36.478975Z"
    }
   },
   "id": "e2bd2cdb44d59a51"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# use glove word embeddings to extract features\n",
    "glove_model = gensim.models.KeyedVectors.load('glove-twitter-200.bin')\n",
    "\n",
    "def get_glove_vector(word):\n",
    "    try:\n",
    "        return glove_model[word]\n",
    "    except:\n",
    "        return np.zeros(200)\n",
    "    \n",
    "def get_glove_sentence_vector(sentence):\n",
    "    words = sentence.split()\n",
    "    vectors = [get_glove_vector(word) for word in words]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "def train_and_test_glove_embedding(title_column):\n",
    "    X = df[title_column]\n",
    "    y = df['label']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5664)\n",
    "    \n",
    "    X_train_glove_embedding = np.array([get_glove_sentence_vector(sentence) for sentence in X_train])\n",
    "    X_test_glove_embedding = np.array([get_glove_sentence_vector(sentence) for sentence in X_test])\n",
    "    \n",
    "    print(f'For {title_column} (original data):')\n",
    "    train_and_test('Glove Embedding', X_train_glove_embedding, X_test_glove_embedding, y_train, y_test)\n",
    "    \n",
    "    # use same model and test on df_fnn\n",
    "    X_test = df_fnn[title_column]\n",
    "    y_test = df_fnn['label']\n",
    "    \n",
    "    X_test_glove_embedding = np.array([get_glove_sentence_vector(sentence) for sentence in X_test])\n",
    "    \n",
    "    print(f'For {title_column} (FakeNewsNet data):')\n",
    "    train_and_test('Glove Embedding', X_train_glove_embedding, X_test_glove_embedding, y_train, y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T17:09:01.880692Z",
     "start_time": "2024-11-17T17:09:01.112614Z"
    }
   },
   "id": "f5cbdd90fe8a6fba"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For title_clean (original data):\n",
      "Feature selection method: Glove Embedding\n",
      "LogisticRegression: 90.05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90      4258\n",
      "           1       0.91      0.90      0.91      4720\n",
      "\n",
      "    accuracy                           0.90      8978\n",
      "   macro avg       0.90      0.90      0.90      8978\n",
      "weighted avg       0.90      0.90      0.90      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 82.35\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81      4258\n",
      "           1       0.82      0.85      0.84      4720\n",
      "\n",
      "    accuracy                           0.82      8978\n",
      "   macro avg       0.82      0.82      0.82      8978\n",
      "weighted avg       0.82      0.82      0.82      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 90.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91      4258\n",
      "           1       0.92      0.90      0.91      4720\n",
      "\n",
      "    accuracy                           0.91      8978\n",
      "   macro avg       0.91      0.91      0.91      8978\n",
      "weighted avg       0.91      0.91      0.91      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 92.39\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      4258\n",
      "           1       0.93      0.92      0.93      4720\n",
      "\n",
      "    accuracy                           0.92      8978\n",
      "   macro avg       0.92      0.92      0.92      8978\n",
      "weighted avg       0.92      0.92      0.92      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean (FakeNewsNet data):\n",
      "Feature selection method: Glove Embedding\n",
      "LogisticRegression: 69.59\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.07      0.11      5754\n",
      "           1       0.75      0.90      0.82     17438\n",
      "\n",
      "    accuracy                           0.70     23192\n",
      "   macro avg       0.47      0.49      0.46     23192\n",
      "weighted avg       0.61      0.70      0.64     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 57.74\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.36      0.30      5754\n",
      "           1       0.75      0.65      0.70     17438\n",
      "\n",
      "    accuracy                           0.58     23192\n",
      "   macro avg       0.50      0.50      0.50     23192\n",
      "weighted avg       0.63      0.58      0.60     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 66.55\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.14      0.17      5754\n",
      "           1       0.75      0.84      0.79     17438\n",
      "\n",
      "    accuracy                           0.67     23192\n",
      "   macro avg       0.48      0.49      0.48     23192\n",
      "weighted avg       0.62      0.67      0.64     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 62.06\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.22      0.22      5754\n",
      "           1       0.75      0.75      0.75     17438\n",
      "\n",
      "    accuracy                           0.62     23192\n",
      "   macro avg       0.49      0.49      0.49     23192\n",
      "weighted avg       0.62      0.62      0.62     23192\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test with title_clean\n",
    "train_and_test_glove_embedding('title_clean')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T17:12:48.814930Z",
     "start_time": "2024-11-17T17:09:05.267953Z"
    }
   },
   "id": "37a43b6e5e350d74"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For title_clean_ner (original data):\n",
      "Feature selection method: Glove Embedding\n",
      "LogisticRegression: 87.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87      4258\n",
      "           1       0.88      0.88      0.88      4720\n",
      "\n",
      "    accuracy                           0.87      8978\n",
      "   macro avg       0.87      0.87      0.87      8978\n",
      "weighted avg       0.87      0.87      0.87      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 79.36\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.75      0.78      4258\n",
      "           1       0.79      0.83      0.81      4720\n",
      "\n",
      "    accuracy                           0.79      8978\n",
      "   macro avg       0.79      0.79      0.79      8978\n",
      "weighted avg       0.79      0.79      0.79      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 89.15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.89      4258\n",
      "           1       0.91      0.88      0.89      4720\n",
      "\n",
      "    accuracy                           0.89      8978\n",
      "   macro avg       0.89      0.89      0.89      8978\n",
      "weighted avg       0.89      0.89      0.89      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 90.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90      4258\n",
      "           1       0.91      0.90      0.90      4720\n",
      "\n",
      "    accuracy                           0.90      8978\n",
      "   macro avg       0.90      0.90      0.90      8978\n",
      "weighted avg       0.90      0.90      0.90      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean_ner (FakeNewsNet data):\n",
      "Feature selection method: Glove Embedding\n",
      "LogisticRegression: 65.54\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.20      0.23      5754\n",
      "           1       0.75      0.80      0.78     17438\n",
      "\n",
      "    accuracy                           0.66     23192\n",
      "   macro avg       0.51      0.50      0.50     23192\n",
      "weighted avg       0.63      0.66      0.64     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 56.39\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.38      0.30      5754\n",
      "           1       0.75      0.63      0.68     17438\n",
      "\n",
      "    accuracy                           0.56     23192\n",
      "   macro avg       0.50      0.50      0.49     23192\n",
      "weighted avg       0.63      0.56      0.59     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 62.72\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.21      0.22      5754\n",
      "           1       0.75      0.77      0.76     17438\n",
      "\n",
      "    accuracy                           0.63     23192\n",
      "   macro avg       0.49      0.49      0.49     23192\n",
      "weighted avg       0.62      0.63      0.62     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 58.33\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.33      0.28      5754\n",
      "           1       0.75      0.67      0.71     17438\n",
      "\n",
      "    accuracy                           0.58     23192\n",
      "   macro avg       0.50      0.50      0.49     23192\n",
      "weighted avg       0.63      0.58      0.60     23192\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test with title_clean_ner\n",
    "train_and_test_glove_embedding('title_clean_ner')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T17:17:36.715583Z",
     "start_time": "2024-11-17T17:13:45.717751Z"
    }
   },
   "id": "2f05643757dcea16"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# use Sentence Vectors embeddings to extract features\n",
    "use_model = hub.load('universal-sentence-encoder')\n",
    "\n",
    "def get_use_vector(sentence):\n",
    "    return use_model([sentence])[0].numpy()\n",
    "\n",
    "def train_and_test_use_embedding(title_column):\n",
    "    X = df[title_column]\n",
    "    y = df['label']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5664)\n",
    "    \n",
    "    X_train_use_embedding = np.array([get_use_vector(sentence) for sentence in X_train])\n",
    "    X_test_use_embedding = np.array([get_use_vector(sentence) for sentence in X_test])\n",
    "    \n",
    "    print(f'For {title_column} (original data):')\n",
    "    train_and_test('USE Embedding', X_train_use_embedding, X_test_use_embedding, y_train, y_test)\n",
    "    \n",
    "    # use same model and test on df_fnn\n",
    "    X_test = df_fnn[title_column]\n",
    "    y_test = df_fnn['label']\n",
    "    \n",
    "    X_test_use_embedding = np.array([get_use_vector(sentence) for sentence in X_test])\n",
    "    \n",
    "    print(f'For {title_column} (FakeNewsNet data):')\n",
    "    train_and_test('USE Embedding', X_train_use_embedding, X_test_use_embedding, y_train, y_test)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T02:27:21.344411Z",
     "start_time": "2024-11-18T02:27:18.631003Z"
    }
   },
   "id": "441bc931bb3a1ab1"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For title_clean (original data):\n",
      "Feature selection method: USE Embedding\n",
      "LogisticRegression: 91.15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91      4258\n",
      "           1       0.92      0.91      0.92      4720\n",
      "\n",
      "    accuracy                           0.91      8978\n",
      "   macro avg       0.91      0.91      0.91      8978\n",
      "weighted avg       0.91      0.91      0.91      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 84.65\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83      4258\n",
      "           1       0.84      0.88      0.86      4720\n",
      "\n",
      "    accuracy                           0.85      8978\n",
      "   macro avg       0.85      0.84      0.85      8978\n",
      "weighted avg       0.85      0.85      0.85      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 92.31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      4258\n",
      "           1       0.93      0.92      0.93      4720\n",
      "\n",
      "    accuracy                           0.92      8978\n",
      "   macro avg       0.92      0.92      0.92      8978\n",
      "weighted avg       0.92      0.92      0.92      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 94.30\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      4258\n",
      "           1       0.95      0.94      0.95      4720\n",
      "\n",
      "    accuracy                           0.94      8978\n",
      "   macro avg       0.94      0.94      0.94      8978\n",
      "weighted avg       0.94      0.94      0.94      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean (FakeNewsNet data):\n",
      "Feature selection method: USE Embedding\n",
      "LogisticRegression: 68.06\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.11      0.15      5754\n",
      "           1       0.75      0.87      0.80     17438\n",
      "\n",
      "    accuracy                           0.68     23192\n",
      "   macro avg       0.48      0.49      0.48     23192\n",
      "weighted avg       0.62      0.68      0.64     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 59.69\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.28      0.26      5754\n",
      "           1       0.75      0.70      0.72     17438\n",
      "\n",
      "    accuracy                           0.60     23192\n",
      "   macro avg       0.49      0.49      0.49     23192\n",
      "weighted avg       0.62      0.60      0.61     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 64.93\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.15      0.18      5754\n",
      "           1       0.74      0.81      0.78     17438\n",
      "\n",
      "    accuracy                           0.65     23192\n",
      "   macro avg       0.48      0.48      0.48     23192\n",
      "weighted avg       0.61      0.65      0.63     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 66.86\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.14      0.18      5754\n",
      "           1       0.75      0.84      0.79     17438\n",
      "\n",
      "    accuracy                           0.67     23192\n",
      "   macro avg       0.49      0.49      0.48     23192\n",
      "weighted avg       0.62      0.67      0.64     23192\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test with title_clean\n",
    "train_and_test_use_embedding('title_clean')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T02:39:26.091722Z",
     "start_time": "2024-11-18T02:27:24.107585Z"
    }
   },
   "id": "15da10024ba5f178"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For title_clean_ner (original data):\n",
      "Feature selection method: USE Embedding\n",
      "LogisticRegression: 89.27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89      4258\n",
      "           1       0.91      0.89      0.90      4720\n",
      "\n",
      "    accuracy                           0.89      8978\n",
      "   macro avg       0.89      0.89      0.89      8978\n",
      "weighted avg       0.89      0.89      0.89      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 82.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.82      4258\n",
      "           1       0.83      0.85      0.84      4720\n",
      "\n",
      "    accuracy                           0.83      8978\n",
      "   macro avg       0.83      0.83      0.83      8978\n",
      "weighted avg       0.83      0.83      0.83      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 90.54\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90      4258\n",
      "           1       0.93      0.89      0.91      4720\n",
      "\n",
      "    accuracy                           0.91      8978\n",
      "   macro avg       0.91      0.91      0.91      8978\n",
      "weighted avg       0.91      0.91      0.91      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 92.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      4258\n",
      "           1       0.94      0.92      0.93      4720\n",
      "\n",
      "    accuracy                           0.93      8978\n",
      "   macro avg       0.93      0.93      0.93      8978\n",
      "weighted avg       0.93      0.93      0.93      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean_ner (FakeNewsNet data):\n",
      "Feature selection method: USE Embedding\n",
      "LogisticRegression: 64.69\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.21      0.23      5754\n",
      "           1       0.75      0.79      0.77     17438\n",
      "\n",
      "    accuracy                           0.65     23192\n",
      "   macro avg       0.50      0.50      0.50     23192\n",
      "weighted avg       0.63      0.65      0.64     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 56.95\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.32      0.27      5754\n",
      "           1       0.74      0.65      0.69     17438\n",
      "\n",
      "    accuracy                           0.57     23192\n",
      "   macro avg       0.49      0.49      0.48     23192\n",
      "weighted avg       0.62      0.57      0.59     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 62.36\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.21      0.22      5754\n",
      "           1       0.74      0.76      0.75     17438\n",
      "\n",
      "    accuracy                           0.62     23192\n",
      "   macro avg       0.49      0.49      0.49     23192\n",
      "weighted avg       0.62      0.62      0.62     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 63.95\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.21      0.22      5754\n",
      "           1       0.75      0.78      0.77     17438\n",
      "\n",
      "    accuracy                           0.64     23192\n",
      "   macro avg       0.49      0.50      0.49     23192\n",
      "weighted avg       0.62      0.64      0.63     23192\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test with title_clean_ner\n",
    "train_and_test_use_embedding('title_clean_ner')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T02:54:44.941254Z",
     "start_time": "2024-11-18T02:41:48.883496Z"
    }
   },
   "id": "ae1e45bb4f33ba8b"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# use VAD score to extract features\n",
    "\n",
    "# load NRC VAD lexicon\n",
    "nrc_vad = pd.read_csv('NRC-VAD-Lexicon.txt', sep='\\t')\n",
    "nrc_vad.columns = ['word', 'valence', 'arousal', 'dominance']\n",
    "\n",
    "# create dictionary for VAD scores\n",
    "vad_dict = {}\n",
    "for i in range(len(nrc_vad)):\n",
    "    word = nrc_vad.iloc[i]['word']\n",
    "    valence = nrc_vad.iloc[i]['valence']\n",
    "    arousal = nrc_vad.iloc[i]['arousal']\n",
    "    dominance = nrc_vad.iloc[i]['dominance']\n",
    "    vad_dict[word] = (valence, arousal, dominance)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T20:54:36.732379Z",
     "start_time": "2024-11-19T20:54:35.172461Z"
    }
   },
   "id": "4b9365277c0a833d"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "vad_scores_title_original = []\n",
    "\n",
    "for title in df['title_clean']:\n",
    "    valence = 0\n",
    "    arousal = 0\n",
    "    dominance = 0\n",
    "    words = title.split()\n",
    "    word_count = 0\n",
    "    for word in words:\n",
    "        if word in vad_dict:\n",
    "            valence += vad_dict[word][0]\n",
    "            arousal += vad_dict[word][1]\n",
    "            dominance += vad_dict[word][2]\n",
    "            word_count += 1\n",
    "    \n",
    "    if word_count == 0:\n",
    "        vad_scores_title_original.append((0, 0, 0))\n",
    "        continue\n",
    "    valence /= word_count\n",
    "    arousal /= word_count\n",
    "    dominance /= word_count\n",
    "    vad_scores_title_original.append((valence, arousal, dominance))\n",
    "\n",
    "df['title_clean_valence'] = [vad_scores_title_original[i][0] for i in range(len(vad_scores_title_original))]\n",
    "df['title_clean_arousal'] = [vad_scores_title_original[i][1] for i in range(len(vad_scores_title_original))]\n",
    "df['title_clean_dominance'] = [vad_scores_title_original[i][2] for i in range(len(vad_scores_title_original))]\n",
    "\n",
    "\n",
    "vad_scores_title_ner_original = []\n",
    "\n",
    "for title in df['title_clean_ner']:\n",
    "    valence = 0\n",
    "    arousal = 0\n",
    "    dominance = 0\n",
    "    words = title.split()\n",
    "    word_count = 0\n",
    "    for word in words:\n",
    "        if word in vad_dict:\n",
    "            valence += vad_dict[word][0]\n",
    "            arousal += vad_dict[word][1]\n",
    "            dominance += vad_dict[word][2]\n",
    "            word_count += 1\n",
    "    \n",
    "    if word_count == 0:\n",
    "        vad_scores_title_ner_original.append((0, 0, 0))\n",
    "        continue\n",
    "    valence /= word_count\n",
    "    arousal /= word_count\n",
    "    dominance /= word_count\n",
    "    vad_scores_title_ner_original.append((valence, arousal, dominance))\n",
    "    \n",
    "df['vad_title_ner_valence'] = [vad_scores_title_ner_original[i][0] for i in range(len(vad_scores_title_ner_original))]\n",
    "df['vad_title_ner_arousal'] = [vad_scores_title_ner_original[i][1] for i in range(len(vad_scores_title_ner_original))]\n",
    "df['vad_title_ner_dominance'] = [vad_scores_title_ner_original[i][2] for i in range(len(vad_scores_title_ner_original))]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T21:05:50.367314Z",
     "start_time": "2024-11-19T21:05:50.072560Z"
    }
   },
   "id": "cdac85aca0ac24c"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# apply VAD score to df_fnn\n",
    "\n",
    "vad_scores_title_fnn = []\n",
    "\n",
    "for title in df_fnn['title_clean']:\n",
    "    valence = 0\n",
    "    arousal = 0\n",
    "    dominance = 0\n",
    "    words = title.split()\n",
    "    word_count = 0\n",
    "    for word in words:\n",
    "        if word in vad_dict:\n",
    "            valence += vad_dict[word][0]\n",
    "            arousal += vad_dict[word][1]\n",
    "            dominance += vad_dict[word][2]\n",
    "            word_count += 1\n",
    "    \n",
    "    if word_count == 0:\n",
    "        vad_scores_title_fnn.append((0, 0, 0))\n",
    "        continue\n",
    "    valence /= word_count\n",
    "    arousal /= word_count\n",
    "    dominance /= word_count\n",
    "    vad_scores_title_fnn.append((valence, arousal, dominance))\n",
    "    \n",
    "df_fnn['title_clean_valence'] = [vad_scores_title_fnn[i][0] for i in range(len(vad_scores_title_fnn))]\n",
    "df_fnn['title_clean_arousal'] = [vad_scores_title_fnn[i][1] for i in range(len(vad_scores_title_fnn))]\n",
    "df_fnn['title_clean_dominance'] = [vad_scores_title_fnn[i][2] for i in range(len(vad_scores_title_fnn))]\n",
    "\n",
    "\n",
    "vad_scores_title_fnn_ner = []\n",
    "\n",
    "for title in df_fnn['title_clean_ner']:\n",
    "    valence = 0\n",
    "    arousal = 0\n",
    "    dominance = 0\n",
    "    words = title.split()\n",
    "    word_count = 0\n",
    "    for word in words:\n",
    "        if word in vad_dict:\n",
    "            valence += vad_dict[word][0]\n",
    "            arousal += vad_dict[word][1]\n",
    "            dominance += vad_dict[word][2]\n",
    "            word_count += 1\n",
    "    \n",
    "    if word_count == 0:\n",
    "        vad_scores_title_fnn_ner.append((0, 0, 0))\n",
    "        continue\n",
    "    valence /= word_count\n",
    "    arousal /= word_count\n",
    "    dominance /= word_count\n",
    "    vad_scores_title_fnn_ner.append((valence, arousal, dominance))\n",
    "\n",
    "df_fnn['vad_title_ner_valence'] = [vad_scores_title_fnn_ner[i][0] for i in range(len(vad_scores_title_fnn_ner))]\n",
    "df_fnn['vad_title_ner_arousal'] = [vad_scores_title_fnn_ner[i][1] for i in range(len(vad_scores_title_fnn_ner))]\n",
    "df_fnn['vad_title_ner_dominance'] = [vad_scores_title_fnn_ner[i][2] for i in range(len(vad_scores_title_fnn_ner))]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T21:26:18.604736Z",
     "start_time": "2024-11-19T21:26:18.480475Z"
    }
   },
   "id": "fd0a8e542b3215bd"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection method: VAD Score\n",
      "LogisticRegression: 64.35\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.59      0.61      4258\n",
      "           1       0.65      0.69      0.67      4720\n",
      "\n",
      "    accuracy                           0.64      8978\n",
      "   macro avg       0.64      0.64      0.64      8978\n",
      "weighted avg       0.64      0.64      0.64      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 65.73\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.60      0.62      4258\n",
      "           1       0.66      0.71      0.68      4720\n",
      "\n",
      "    accuracy                           0.66      8978\n",
      "   macro avg       0.66      0.65      0.65      8978\n",
      "weighted avg       0.66      0.66      0.66      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 70.35\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.65      0.68      4258\n",
      "           1       0.71      0.75      0.73      4720\n",
      "\n",
      "    accuracy                           0.70      8978\n",
      "   macro avg       0.70      0.70      0.70      8978\n",
      "weighted avg       0.70      0.70      0.70      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 65.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.58      0.61      4258\n",
      "           1       0.65      0.71      0.68      4720\n",
      "\n",
      "    accuracy                           0.65      8978\n",
      "   macro avg       0.65      0.65      0.65      8978\n",
      "weighted avg       0.65      0.65      0.65      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean (FakeNewsNet data):\n",
      "Feature selection method: VAD Score\n",
      "LogisticRegression: 65.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.20      0.23      5754\n",
      "           1       0.75      0.81      0.78     17438\n",
      "\n",
      "    accuracy                           0.66     23192\n",
      "   macro avg       0.51      0.51      0.50     23192\n",
      "weighted avg       0.63      0.66      0.64     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 50.91\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.48      0.33      5754\n",
      "           1       0.75      0.52      0.61     17438\n",
      "\n",
      "    accuracy                           0.51     23192\n",
      "   macro avg       0.50      0.50      0.47     23192\n",
      "weighted avg       0.63      0.51      0.54     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 56.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.35      0.28      5754\n",
      "           1       0.75      0.63      0.68     17438\n",
      "\n",
      "    accuracy                           0.56     23192\n",
      "   macro avg       0.49      0.49      0.48     23192\n",
      "weighted avg       0.62      0.56      0.58     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 62.77\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.23      0.23      5754\n",
      "           1       0.75      0.76      0.75     17438\n",
      "\n",
      "    accuracy                           0.63     23192\n",
      "   macro avg       0.49      0.49      0.49     23192\n",
      "weighted avg       0.62      0.63      0.62     23192\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train and test with VAD score for original data\n",
    "X = df[['title_clean_valence', 'title_clean_arousal', 'title_clean_dominance']]\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5664)\n",
    "\n",
    "train_and_test('VAD Score', X_train, X_test, y_train, y_test)\n",
    "\n",
    "# use fnn data as test data\n",
    "X_test = df_fnn[['title_clean_valence', 'title_clean_arousal', 'title_clean_dominance']]\n",
    "y_test = df_fnn['label']\n",
    "\n",
    "print('For title_clean (FakeNewsNet data):')\n",
    "\n",
    "train_and_test('VAD Score', X_train, X_test, y_train, y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T21:20:13.102970Z",
     "start_time": "2024-11-19T21:19:00.713600Z"
    }
   },
   "id": "9b0166c2a5d10e2d"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection method: VAD Score with NER\n",
      "LogisticRegression: 63.71\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.59      0.61      4258\n",
      "           1       0.65      0.68      0.66      4720\n",
      "\n",
      "    accuracy                           0.64      8978\n",
      "   macro avg       0.64      0.63      0.63      8978\n",
      "weighted avg       0.64      0.64      0.64      8978\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 65.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.60      0.62      4258\n",
      "           1       0.66      0.70      0.68      4720\n",
      "\n",
      "    accuracy                           0.65      8978\n",
      "   macro avg       0.65      0.65      0.65      8978\n",
      "weighted avg       0.65      0.65      0.65      8978\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 69.74\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.65      0.67      4258\n",
      "           1       0.70      0.74      0.72      4720\n",
      "\n",
      "    accuracy                           0.70      8978\n",
      "   macro avg       0.70      0.70      0.70      8978\n",
      "weighted avg       0.70      0.70      0.70      8978\n",
      "\n",
      "------------------------------\n",
      "SVC: 64.96\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.58      0.61      4258\n",
      "           1       0.65      0.71      0.68      4720\n",
      "\n",
      "    accuracy                           0.65      8978\n",
      "   macro avg       0.65      0.65      0.65      8978\n",
      "weighted avg       0.65      0.65      0.65      8978\n",
      "\n",
      "------------------------------\n",
      "For title_clean_ner (FakeNewsNet data):\n",
      "Feature selection method: VAD Score with NER\n",
      "LogisticRegression: 66.10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.18      0.21      5754\n",
      "           1       0.75      0.82      0.78     17438\n",
      "\n",
      "    accuracy                           0.66     23192\n",
      "   macro avg       0.50      0.50      0.50     23192\n",
      "weighted avg       0.63      0.66      0.64     23192\n",
      "\n",
      "------------------------------\n",
      "DecisionTreeClassifier: 50.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.48      0.32      5754\n",
      "           1       0.75      0.51      0.61     17438\n",
      "\n",
      "    accuracy                           0.50     23192\n",
      "   macro avg       0.50      0.49      0.46     23192\n",
      "weighted avg       0.62      0.50      0.54     23192\n",
      "\n",
      "------------------------------\n",
      "RandomForestClassifier: 52.91\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.43      0.31      5754\n",
      "           1       0.75      0.56      0.64     17438\n",
      "\n",
      "    accuracy                           0.53     23192\n",
      "   macro avg       0.50      0.49      0.48     23192\n",
      "weighted avg       0.62      0.53      0.56     23192\n",
      "\n",
      "------------------------------\n",
      "SVC: 56.33\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.35      0.29      5754\n",
      "           1       0.75      0.63      0.69     17438\n",
      "\n",
      "    accuracy                           0.56     23192\n",
      "   macro avg       0.49      0.49      0.49     23192\n",
      "weighted avg       0.62      0.56      0.59     23192\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train and test with VAD score for original data with ner\n",
    "X = df[['vad_title_ner_valence', 'vad_title_ner_arousal', 'vad_title_ner_dominance']]\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5664)\n",
    "\n",
    "train_and_test('VAD Score with NER', X_train, X_test, y_train, y_test)\n",
    "\n",
    "# use fnn data as test data\n",
    "X_test = df_fnn[['vad_title_ner_valence', 'vad_title_ner_arousal', 'vad_title_ner_dominance']]\n",
    "y_test = df_fnn['label']\n",
    "\n",
    "print('For title_clean_ner (FakeNewsNet data):')\n",
    "\n",
    "train_and_test('VAD Score with NER', X_train, X_test, y_train, y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T21:27:35.521931Z",
     "start_time": "2024-11-19T21:26:25.458706Z"
    }
   },
   "id": "b2890679a390eba7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a70ddd14dc65cbc5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
